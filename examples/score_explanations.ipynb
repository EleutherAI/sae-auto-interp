{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring explanations after generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will show a simple example of how to score the explanations generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import os   \n",
    "import torch\n",
    "import orjson\n",
    "import asyncio\n",
    "from delphi.clients import OpenRouter\n",
    "from delphi.config import ExperimentConfig, LatentConfig\n",
    "from delphi.explainers import explanation_loader\n",
    "from delphi.latents import (\n",
    "    LatentDataset,\n",
    "    LatentLoader\n",
    ")\n",
    "from delphi.latents.constructors import default_constructor\n",
    "from delphi.latents.samplers import sample\n",
    "from delphi.pipeline import Pipeline, process_wrapper\n",
    "from delphi.scorers import FuzzingScorer\n",
    "\n",
    "\n",
    "\n",
    "API_KEY = os.getenv(\"OPENROUTER_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_cfg = LatentConfig(\n",
    "    width=131072, # The number of latents of your SAE\n",
    "    min_examples=200, # The minimum number of examples to consider for the latent to be explained\n",
    "    max_examples=10000, # The maximum number of examples to be sampled from\n",
    "    n_splits=5 # How many splits was the cache split into\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = \".model.layers.10\" # The layer to score\n",
    "latent_dict = {module: torch.arange(0,3)} # The what latents to score\n",
    "\n",
    "dataset = LatentDataset(\n",
    "        raw_dir=\"latents\", # The folder where the cache is stored\n",
    "        cfg=latent_cfg,\n",
    "        modules=[module],\n",
    "        latents=latent_dict,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the config for the examples shown to the scorer model.\n",
    "When selecting the examples to be shown to the scorer model we can select them from:\n",
    "- \"quantiles\", which gets examples from the quantiles of the data\n",
    "- \"activations\", which gets examples from activation bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "experiment_cfg = ExperimentConfig(\n",
    "    n_examples_test=10, # Number of examples to sample for testing\n",
    "    n_quantiles=10, # Number of quantiles to divide the data into\n",
    "    test_type=\"quantiles\", # Type of sampler to use for testing. \n",
    "    n_non_activating=10, # Number of non-activating examples to sample\n",
    "    example_ctx_len=32, # Length of each example\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constructor and sampler here are the same as the ones used in the generation of the explanations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "constructor=partial(\n",
    "            default_constructor,\n",
    "            token_loader=None,\n",
    "            n_not_active=experiment_cfg.n_non_activating, \n",
    "            ctx_len=experiment_cfg.example_ctx_len, \n",
    "            max_examples=latent_cfg.max_examples\n",
    "        )\n",
    "sampler=partial(sample,cfg=experiment_cfg)\n",
    "loader = LatentLoader(dataset, constructor=constructor, sampler=sampler)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we could generate the explanations in the pipeline, here we load the explanations already generated. Then we define the scorer. Because the scorer should use information from the previous pipe, we have a preprocess and a postprocess function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenRouter(\"anthropic/claude-3.5-sonnet\",api_key=API_KEY)\n",
    "\n",
    "# Load the explanations already generated\n",
    "explainer_pipe = partial(explanation_loader, explanation_dir=\"results/explanations\")\n",
    "\n",
    "\n",
    "# Builds the record from result returned by the pipeline\n",
    "def scorer_preprocess(result):\n",
    "        record = result.record   \n",
    "        record.explanation = result.explanation\n",
    "        record.extra_examples = record.not_active\n",
    "\n",
    "        return record\n",
    "\n",
    "# Saves the score to a file\n",
    "def scorer_postprocess(result, score_dir):\n",
    "    with open(f\"results/scores/{result.record.latent}.txt\", \"wb\") as f:\n",
    "        f.write(orjson.dumps(result.score))\n",
    "\n",
    "\n",
    "scorer_pipe = process_wrapper(\n",
    "    FuzzingScorer(client, tokenizer=dataset.tokenizer),\n",
    "    preprocess=scorer_preprocess,\n",
    "    postprocess=partial(scorer_postprocess, score_dir=\"fuzz\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our pipeline only has three steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p results/scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No available randomly sampled non-activating sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items: 2it [00:08,  4.25s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline(\n",
    "    loader,\n",
    "    explainer_pipe,\n",
    "    scorer_pipe,\n",
    ")\n",
    "number_of_parallel_latents = 10\n",
    "await pipeline.run(number_of_parallel_latents) # This will start generating the explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
