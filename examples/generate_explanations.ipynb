{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating explanations after caching the latents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will show a simple example of how to generate explanations for a SAE after caching the latents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from os import getenv\n",
    "\n",
    "API_KEY = getenv(\"OPENROUTER_API_KEY\")\n",
    "import torch\n",
    "import orjson\n",
    "import os\n",
    "from sae_auto_interp.clients import OpenRouter\n",
    "from sae_auto_interp.config import ExperimentConfig, FeatureConfig\n",
    "from sae_auto_interp.explainers import DefaultExplainer\n",
    "from sae_auto_interp.features import (\n",
    "    FeatureDataset,\n",
    "    FeatureLoader\n",
    ")\n",
    "from sae_auto_interp.features.constructors import default_constructor\n",
    "from sae_auto_interp.features.samplers import sample\n",
    "from sae_auto_interp.pipeline import Pipeline, process_wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cfg = FeatureConfig(\n",
    "    width=131072, # The number of latents of your SAE\n",
    "    min_examples=200, # The minimum number of examples to consider for the feature to be explained\n",
    "    max_examples=10000, # The maximum number of examples to be sampled from\n",
    "    n_splits=5 # How many splits was the cache split into\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EleutherAI/rpj-v2-sample  train[:1%]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b8e9188d03a4759a7cafb2c2165ea95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "module = \".model.layers.10\" # The layer to explain\n",
    "feature_dict = {module: torch.arange(0,10)} # The what latents to explain\n",
    "\n",
    "dataset = FeatureDataset(\n",
    "        raw_dir=\"latents\", # The folder where the cache is stored\n",
    "        cfg=feature_cfg,\n",
    "        modules=[module],\n",
    "        features=feature_dict,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the config for the examples shown to the explainer model.\n",
    "When selecting the examples to be shown to the explainer model we can select them from:\n",
    "- \"top\", which gets the most activating examples\n",
    "- \"random\" which gets random examples from the whole activation distribution\n",
    "- \"quantiles\" which gets examples from the quantiles of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "experiment_cfg = ExperimentConfig(\n",
    "    n_examples_train=40, # Number of examples to sample for training\n",
    "    example_ctx_len=32, # Length of each example\n",
    "    train_type=\"random\", # Type of sampler to use for training. \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constructor defines the window of tokens to be used for the examples. We have a default constructor that builds examples of size ctx_len (should be a divisor of the ctx_len used for caching the latents).\n",
    "The sampler defines how the examples are selected. The sampler will always generate a train and test set, but here we only care about the train set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "constructor=partial(\n",
    "            default_constructor,\n",
    "            tokens=dataset.tokens,\n",
    "            n_random=experiment_cfg.n_random, \n",
    "            ctx_len=experiment_cfg.example_ctx_len, \n",
    "            max_examples=feature_cfg.max_examples\n",
    "        )\n",
    "sampler=partial(sample,cfg=experiment_cfg)\n",
    "loader = FeatureLoader(dataset, constructor=constructor, sampler=sampler)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use pipes to generate the explanations. Each pipe starts with loading the examples from the corresponding latent and then passes the examples to the explainer. It used a client (here OpenRouter) to generate the explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenRouter(\"anthropic/claude-3.5-sonnet\",api_key=API_KEY)\n",
    "\n",
    "# The function that saves the explanations\n",
    "def explainer_postprocess(result):\n",
    "        with open(f\"results/explanations/{result.record.feature}.txt\", \"wb\") as f:\n",
    "            f.write(orjson.dumps(result.explanation))\n",
    "        del result\n",
    "        return None\n",
    "\n",
    "explainer_pipe = process_wrapper(\n",
    "        DefaultExplainer(\n",
    "            client, \n",
    "            tokenizer=dataset.tokenizer,\n",
    "        ),\n",
    "        postprocess=explainer_postprocess,\n",
    "    )\n",
    "\n",
    "os.makedirs(\"results/explanations\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are generating only explanations, show our pipeline only has two steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items: 0it [00:00, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to generate text after multiple attempts.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(\n\u001b[1;32m      2\u001b[0m     loader,\n\u001b[1;32m      3\u001b[0m     explainer_pipe,\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m number_of_parallel_latents \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m pipeline\u001b[38;5;241m.\u001b[39mrun(number_of_parallel_latents)\n",
      "File \u001b[0;32m/mnt/ssd-1/gpaulo/SAE-Zoology.worktrees/article_version/sae_auto_interp/pipeline.py:118\u001b[0m, in \u001b[0;36mPipeline.run\u001b[0;34m(self, max_concurrent)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tasks:\n\u001b[1;32m    117\u001b[0m     done, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mwait(tasks)\n\u001b[0;32m--> 118\u001b[0m     \u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m progress_bar\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m/mnt/ssd-1/gpaulo/SAE-Zoology.worktrees/article_version/sae_auto_interp/pipeline.py:118\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tasks:\n\u001b[1;32m    117\u001b[0m     done, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mwait(tasks)\n\u001b[0;32m--> 118\u001b[0m     results\u001b[38;5;241m.\u001b[39mextend(\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m done)\n\u001b[1;32m    120\u001b[0m progress_bar\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m/mnt/ssd-1/gpaulo/SAE-Zoology.worktrees/article_version/sae_auto_interp/pipeline.py:100\u001b[0m, in \u001b[0;36mPipeline.run.<locals>.process_and_update\u001b[0;34m(item, semaphore, count)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_and_update\u001b[39m(item, semaphore, count):\n\u001b[0;32m--> 100\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_item(item,semaphore, count)\n\u001b[1;32m    101\u001b[0m     progress_bar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/mnt/ssd-1/gpaulo/SAE-Zoology.worktrees/article_version/sae_auto_interp/pipeline.py:160\u001b[0m, in \u001b[0;36mPipeline.process_item\u001b[0;34m(self, item, semaphore, count)\u001b[0m\n\u001b[1;32m    158\u001b[0m     result \u001b[38;5;241m=\u001b[39m item\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pipe \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipes[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m--> 160\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m pipe(result)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/mnt/ssd-1/gpaulo/SAE-Zoology.worktrees/article_version/sae_auto_interp/pipeline.py:30\u001b[0m, in \u001b[0;36mprocess_wrapper.<locals>.wrapped\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preprocess \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m preprocess(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m function(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m postprocess \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     results \u001b[38;5;241m=\u001b[39m postprocess(results)\n",
      "File \u001b[0;32m/mnt/ssd-1/gpaulo/SAE-Zoology.worktrees/article_version/sae_auto_interp/explainers/default/default.py:37\u001b[0m, in \u001b[0;36mDefaultExplainer.__call__\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, record):\n\u001b[1;32m     35\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_prompt(record\u001b[38;5;241m.\u001b[39mtrain)\n\u001b[0;32m---> 37\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mgenerate(messages, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_kwargs)\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m         explanation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_explanation(response\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[0;32m/mnt/ssd-1/gpaulo/SAE-Zoology.worktrees/article_version/sae_auto_interp/clients/openrouter.py:71\u001b[0m, in \u001b[0;36mOpenRouter.generate\u001b[0;34m(self, prompt, raw, max_retries, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m sleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     70\u001b[0m logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll retry attempts failed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 71\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to generate text after multiple attempts.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to generate text after multiple attempts."
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(\n",
    "    loader,\n",
    "    explainer_pipe,\n",
    ")\n",
    "number_of_parallel_latents = 10\n",
    "await pipeline.run(number_of_parallel_latents) # This will start generating the explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
